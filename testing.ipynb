{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "from typing import List, Dict, Any, Optional\n",
    "import jinja2\n",
    "import json\n",
    "from abc import ABC, abstractmethod\n",
    "from tavily import TavilyClient\n",
    "import os\n",
    "\n",
    "\n",
    "# Objective: Create a conversation object that can be used to send messages to the model and get responses and tools\n",
    "class Conversation:\n",
    "    def __init__(self, model: str = \"gpt-4o\"):  # gemini/gemini-2.0-flash\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        self.messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def get_response(self, tool_choice: str = \"auto\", tools: Optional[List] = None):\n",
    "        if tools is None:\n",
    "            tool_choice = None\n",
    "        return completion(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "        )\n",
    "\n",
    "\n",
    "# Updated abstract Tool class that enforces `name` and `terminating` and accepts extra kwargs.\n",
    "class Tool(ABC):\n",
    "    def __init__(self, name: str, terminating: bool = False):\n",
    "        self.name = name\n",
    "        self.terminating = terminating\n",
    "\n",
    "    @abstractmethod\n",
    "    def run(self, conversation: Conversation) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_config(self) -> Dict[str, Any]:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Reasoning tool (non‐terminating)\n",
    "class Reasoning(Tool):\n",
    "    def __init__(self, terminating: bool = False):\n",
    "        super().__init__(name=\"reasoning\", terminating=terminating)\n",
    "\n",
    "    def run(self, conversation: Conversation) -> str:\n",
    "        with open(\"templates/reasoning.jinja2\", \"r\") as file:\n",
    "            reasoning_template = jinja2.Template(file.read()).render()\n",
    "        conversation.add_message(\"system\", reasoning_template)\n",
    "        response = conversation.get_response()\n",
    "        conversation.messages.pop()  # remove the reasoning message\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def get_config(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": \"This tool provides reasoning for the given context.\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "# Respond tool (terminating) – note the extra parameter `response_type`\n",
    "class Respond(Tool):\n",
    "    def __init__(self, response_types: List[str], terminating: bool = True):\n",
    "        # Mark this tool as terminating since its response is meant to be final.\n",
    "        super().__init__(name=\"respond\", terminating=terminating)\n",
    "        self.response_types = response_types\n",
    "\n",
    "    def run(self, conversation: Conversation, response_type: str = \"\") -> str:\n",
    "        if not response_type:\n",
    "            response_type = \"respond\"\n",
    "\n",
    "        with open(f\"templates/{response_type}.jinja2\", \"r\") as file:\n",
    "            template_content = jinja2.Template(file.read()).render()\n",
    "        conversation.add_message(\"system\", template_content)\n",
    "        response = conversation.get_response()\n",
    "        conversation.messages.pop()\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": \"This tool provides responses for the given user message.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"response_type\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The type of response to be generated.\",\n",
    "                            \"enum\": self.response_types,\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"response_type\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "# Search tool (non‐terminating)\n",
    "class Search(Tool):\n",
    "    def __init__(self, terminating: bool = False):\n",
    "        super().__init__(name=\"search\", terminating=terminating)\n",
    "        self.tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "\n",
    "    def run(self, conversation: Conversation, query: str) -> str:\n",
    "        response = self.tavily_client.search(query)\n",
    "        return response\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": \"This tool provides search results for the given query.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The query to search for.\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"query\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "# An agent is a tool that can be used to interact with the model and get responses and use tools as required.\n",
    "class Agent(Tool):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        fallback_tool: Tool,\n",
    "        max_turns: int = 3,\n",
    "        terminating: bool = False,\n",
    "        tools: List[Tool] = [],\n",
    "    ):\n",
    "        super().__init__(name=name, terminating=terminating)\n",
    "        self.description = description\n",
    "        self.max_turns = max_turns\n",
    "        self.available_tools = {tool.name: tool for tool in tools}\n",
    "        self.fallback_tool = fallback_tool\n",
    "\n",
    "    def run(self, conversation: Conversation) -> str:\n",
    "        for turn in range(self.max_turns):\n",
    "            tool, function_args = self.get_next_action(conversation)\n",
    "            print(f\"Using the {tool.name} tool with arguments {function_args}\")\n",
    "            response = tool.run(conversation, **function_args)\n",
    "            if tool.terminating:\n",
    "                return response\n",
    "\n",
    "            conversation.add_message(\n",
    "                \"user\",\n",
    "                f\"The {tool.name} tool has been used with args {function_args} and it responded with {response}\",\n",
    "            )\n",
    "        return self.fallback_tool.run(conversation)\n",
    "\n",
    "    def get_next_action(self, conversation: Conversation) -> Tool:\n",
    "        response = conversation.get_response(\n",
    "            tool_choice=\"required\",\n",
    "            tools=[tool.get_config() for tool in self.available_tools.values()],\n",
    "        )\n",
    "        tool_calls = response.choices[0].message.tool_calls\n",
    "        function_name = tool_calls[0].function.name\n",
    "        function_args = json.loads(tool_calls[0].function.arguments)\n",
    "        return self.available_tools[function_name], function_args\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": self.description,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"templates/system_message.jinja2\", \"r\") as file:\n",
    "    system_message = jinja2.Template(file.read()).render(tools=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the reasoning tool with arguments {}\n",
      "Using the respond tool with arguments {'response_type': 'clarification'}\n",
      "Okay, I understand you want to research climate litigation. To make sure I focus on the most relevant information for you, I need to clarify a few things.\n",
      "\n",
      "It sounds like you are interested in a broad overview of climate litigation. To confirm, are you looking to understand:\n",
      "\n",
      "1.  The general landscape of climate litigation, including the types of cases, key players, and legal theories involved?\n",
      "2.  The different jurisdictions where climate litigation is taking place (e.g., US, Europe, specific countries)?\n",
      "3.  The specific types of climate litigation that are being pursued (e.g., cases against governments or fossil fuel companies)?\n",
      "4.  Specific issues or legal theories related to climate litigation, such as attribution science or fiduciary duty?\n",
      "5.  The intended use of your research, such as an internal briefing or academic paper?\n",
      "\n",
      "Please let me know if this accurately reflects your research goals or if there are any adjustments you'd like to make. The more specific you are, the better I can tailor my research to your needs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = Conversation()\n",
    "conversation.add_message(\"system\", system_message)\n",
    "\n",
    "write_report = Respond(response_types=[\"write_report\"], terminating=True)\n",
    "research_agent = Agent(\n",
    "    name=\"research_agent\",\n",
    "    description=\"This agent is designed to help users with research.\",\n",
    "    tools=[Search(), Reasoning(), write_report],\n",
    "    max_turns=15,\n",
    "    terminating=True,\n",
    "    fallback_tool=write_report,\n",
    ")\n",
    "\n",
    "respond = Respond(response_types=[\"respond\", \"clarification\"])\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"main_agent\",\n",
    "    description=\"This agent is designed to help users with market research.\",\n",
    "    tools=[Reasoning(), respond, research_agent],\n",
    "    fallback_tool=respond,\n",
    ")\n",
    "\n",
    "conversation.add_message(\"user\", \"I want to research climate litigation\")\n",
    "\n",
    "\n",
    "response = agent.run(conversation)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the reasoning tool with arguments {}\n",
      "Using the research_agent tool with arguments {}\n",
      "Using the search tool with arguments {'query': 'climate litigation carbon capture technologies'}\n",
      "Using the reasoning tool with arguments {}\n",
      "Using the search tool with arguments {'query': 'climate litigation cases carbon capture'}\n",
      "Using the search tool with arguments {'query': 'climate litigation carbon capture project approvals'}\n",
      "Using the reasoning tool with arguments {}\n",
      "Using the search tool with arguments {'query': 'lawsuit Elk Hills carbon capture project'}\n",
      "Using the reasoning tool with arguments {}\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n    \"status\": \"RESOURCE_EXHAUSTED\"\n  }\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/autonomous-market-research-agent-8W59owSb-py3.13/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:1282\u001b[0m, in \u001b[0;36mVertexLLM.completion\u001b[0;34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/autonomous-market-research-agent-8W59owSb-py3.13/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py:553\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/autonomous-market-research-agent-8W59owSb-py3.13/lib/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py:534\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[0;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[1;32m    533\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msend(req, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[0;32m--> 534\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/autonomous-market-research-agent-8W59owSb-py3.13/lib/python3.13/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyAmsZF06PLEF6592Pyqqvwgq6wxQ_shqws'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mVertexAIError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/autonomous-market-research-agent-8W59owSb-py3.13/lib/python3.13/site-packages/litellm/main.py:2328\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m     new_params \u001b[38;5;241m=\u001b[39m deepcopy(optional_params)\n\u001b[0;32m-> 2328\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mvertex_chat_completion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m   2335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertex_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_ai_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertex_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvertex_credentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertex_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgemini_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgemini_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2350\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/autonomous-market-research-agent-8W59owSb-py3.13/lib/python3.13/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py:1286\u001b[0m, in \u001b[0;36mVertexLLM.completion\u001b[0;34m(self, model, messages, model_response, print_verbose, custom_llm_provider, encoding, logging_obj, optional_params, acompletion, timeout, vertex_project, vertex_location, vertex_credentials, gemini_api_key, litellm_params, logger_fn, extra_headers, client, api_base)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[0;32m-> 1286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VertexAIError(\n\u001b[1;32m   1287\u001b[0m         status_code\u001b[38;5;241m=\u001b[39merror_code,\n\u001b[1;32m   1288\u001b[0m         message\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   1289\u001b[0m         headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1290\u001b[0m     )\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException:\n",
      "\u001b[0;31mVertexAIError\u001b[0m: {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n    \"status\": \"RESOURCE_EXHAUSTED\"\n  }\n}\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m conversation\u001b[38;5;241m.\u001b[39madd_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m      3\u001b[0m conversation\u001b[38;5;241m.\u001b[39madd_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI want to focus on carbon capture technologies for my research. I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm interested in the general landscape, but also the emerging trends.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[0;32mIn[23], line 158\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, conversation)\u001b[0m\n\u001b[1;32m    156\u001b[0m tool, function_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_next_action(conversation)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tool with arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 158\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool\u001b[38;5;241m.\u001b[39mterminating:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[23], line 158\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, conversation)\u001b[0m\n\u001b[1;32m    156\u001b[0m tool, function_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_next_action(conversation)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tool with arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 158\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunction_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tool\u001b[38;5;241m.\u001b[39mterminating:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[23], line 54\u001b[0m, in \u001b[0;36mReasoning.run\u001b[0;34m(self, conversation)\u001b[0m\n\u001b[1;32m     52\u001b[0m     reasoning_template \u001b[38;5;241m=\u001b[39m jinja2\u001b[38;5;241m.\u001b[39mTemplate(file\u001b[38;5;241m.\u001b[39mread())\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m     53\u001b[0m conversation\u001b[38;5;241m.\u001b[39madd_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, reasoning_template)\n\u001b[0;32m---> 54\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mconversation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m conversation\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mpop()  \u001b[38;5;66;03m# remove the reasoning message\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "Cell \u001b[0;32mIn[23], line 22\u001b[0m, in \u001b[0;36mConversation.get_response\u001b[0;34m(self, tool_choice, tools)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     tool_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/autonomous-market-research-agent-8W59owSb-py3.13/lib/python3.13/site-packages/litellm/utils.py:1154\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[1;32m   1151\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[1;32m   1152\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[1;32m   1153\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[0;32m-> 1154\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/autonomous-market-research-agent-8W59owSb-py3.13/lib/python3.13/site-packages/litellm/utils.py:1032\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[0;32m-> 1032\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/autonomous-market-research-agent-8W59owSb-py3.13/lib/python3.13/site-packages/litellm/main.py:3085\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   3082\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3084\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[0;32m-> 3085\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3086\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3088\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3089\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/autonomous-market-research-agent-8W59owSb-py3.13/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2201\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[0;32m-> 2201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/autonomous-market-research-agent-8W59owSb-py3.13/lib/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:1243\u001b[0m, in \u001b[0;36mexception_type\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m429\u001b[39m:\n\u001b[1;32m   1242\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[1;32m   1244\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm.RateLimitError: VertexAIException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1245\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1246\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1247\u001b[0m         litellm_debug_info\u001b[38;5;241m=\u001b[39mextra_information,\n\u001b[1;32m   1248\u001b[0m         response\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mResponse(\n\u001b[1;32m   1249\u001b[0m             status_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m429\u001b[39m,\n\u001b[1;32m   1250\u001b[0m             request\u001b[38;5;241m=\u001b[39mhttpx\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m   1251\u001b[0m                 method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1252\u001b[0m                 url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://cloud.google.com/vertex-ai/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1253\u001b[0m             ),\n\u001b[1;32m   1254\u001b[0m         ),\n\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m   1257\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {\n  \"error\": {\n    \"code\": 429,\n    \"message\": \"Resource has been exhausted (e.g. check quota).\",\n    \"status\": \"RESOURCE_EXHAUSTED\"\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "conversation.add_message(\"assistant\", response)\n",
    "\n",
    "conversation.add_message(\"user\", \"I want to focus on carbon capture technologies for my research. I'm interested in the general landscape, but also the emerging trends.\")\n",
    "\n",
    "\n",
    "response = agent.run(conversation)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the reasoning tool with arguments {}\n",
      "Using the research_agent tool with arguments {}\n",
      "Using the search tool with arguments {'query': 'recent developments climate litigation carbon capture carbon removal \"carbon removal efficacy\" OR \"ability to remove carbon\"'}\n",
      "Using the search tool with arguments {'query': 'recent developments in climate litigation carbon capture \"carbon removal efficacy\" OR \"ability to remove carbon\"'}\n",
      "Using the reasoning tool with arguments {}\n",
      "```tool_code\n",
      "print(research.search(query=\"climate litigation carbon capture projects legal challenges disputes regulations \\\"carbon removal efficacy\\\" OR \\\"carbon capture viability\\\" OR \\\"carbon capture performance\\\"\"))\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "conversation.add_message(\"assistant\", response)\n",
    "\n",
    "conversation.add_message(\"user\", \"Global. No particular parties. No claims of interest, except ability to remove carbon from the atmosphere. Give me the recent developments. I want a high level overview and in-depth analysis. Please, now research.\")\n",
    "\n",
    "\n",
    "response = agent.run(conversation)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autonomous-market-research-agent-8W59owSb-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
